# AI-Research-Assistant-üìò Advanced AI Research Assistant: Hybrid Retrieval and LangGraph OrchestrationA sophisticated Streamlit application for AI-powered PDF document analysis, Q&A, Summarization, and Comparative Review, built on state-of-the-art RAG techniques.This project showcases expertise in building robust, multi-step Generative AI applications using Streamlit for a responsive user interface and a powerful LangGraph workflow for reliable backend intelligence. It addresses the critical need for grounded, verifiable answers in academic research by employing a dedicated Hybrid Retrieval Augmented Generation (RAG) pipeline.üéØ Project Motivation & Technical ChallengeProblem Statement (Academic Context)Research often involves synthesizing information from multiple, lengthy PDF documents. Traditional search is inefficient, and standard AI assistants frequently hallucinate or provide answers lacking citations. The user needs a tool that can provide highly accurate, grounded, and citable answers specific to their uploaded document set.Technical Challenge (Solution & Expertise)The core challenge was ensuring the model received the most relevant context possible, even across large, semantically diverse documents. This required moving beyond basic vector search to implement an advanced pipeline:Hybrid Search: Combining the semantic precision of Dense Retrieval (Qdrant) with the keyword exactness of Sparse Retrieval (BM25) to cover all query types.Cross-Encoder Reranking: Implementing a powerful Cross-Encoder model after the initial retrieval to re-score and eliminate low-relevance document chunks. This crucial step minimizes noise and significantly boosts the quality and reliability of the final answer.Stateful Workflow: Orchestrating complex, multi-step tasks (like comparative analysis and summarization) reliably using LangGraph to manage application state and sequence decisions seamlessly.‚ú® Key FeaturesPDF Upload & Vectorization: Easily upload single or multiple PDF research papers. Documents are automatically chunked and indexed using OpenAI Embeddings and stored in a Qdrant vector database.Hybrid Retrieval (RAG): Combines Dense Retrieval (Qdrant) and Sparse Retrieval (BM25) for highly relevant context extraction, followed by Cross-Encoder Reranking to ensure the most accurate context is passed to the LLM.Intelligent Q&A: Ask complex questions about the uploaded document(s) and receive answers strictly grounded in the content, complete with inline citations.Research Tools Suite:Summarization (Literature Review Mode): Generate abstracts, critical analyses, or key bullet points of documents.Comparative Analysis: Upload two papers and generate a structured report comparing their methodologies, findings, and contributions.Citation Generator: Instantly generate bibliographic citations in popular formats (APA, IEEE, MLA, Chicago).Modular Architecture: Uses LangGraph for a clear, stateful workflow management across different research tasks, ensuring low latency and high reliability.üõ†Ô∏è Technology StackComponentTechnologyRoleFrontendstreamlitUser interface and document upload management.Workflow OrchestrationlanggraphState management and orchestration of complex research flows.LLM Interfacelangchain_openai, ChatOpenAIHandles interaction with gpt-4o-mini for reasoning and generation.Vector DatabaseQdrantHighly scalable vector storage for semantic search and payload filtering.EmbeddingsOpenAIEmbeddingsConverts text chunks into vector representations.RetrievalBM25Retriever, EnsembleRetriever, CrossEncoderHybrid search and reranking for superior context quality.üí° Key Learnings & Skill ShowcaseThis project served as a deep dive into advanced GenAI application development, yielding expertise in:Advanced RAG Pipeline Design: Successfully implementing and validating the performance gain from a hybrid retrieval approach combined with reranking over basic vector search.Stateful Agentic Workflows (LangGraph): Mastering how to define state, nodes, and conditional edges (route_task in utils.py) to handle complex, branching application logic (Q&A vs. Summarization vs. Comparison).Integration of Disparate Services: Seamlessly connecting LangChain components with a cloud-based Vector DB (Qdrant) and a high-performance web framework (Streamlit).PDF Processing: Developing robust document loading and chunking strategies (load_and_split_pdf in utils.py) that preserve crucial metadata (like source) for accurate citation generation.üöÄ Getting StartedPrerequisitesPython 3.8+An OpenAI API Key (required for LLM and Embeddings).A Qdrant Host URL and API Key (you can use Qdrant Cloud or a self-hosted instance).1. Environment SetupCreate a file named .env in the root directory and populate it with your API keys:# .env file
OPENAI_API_KEY="sk-..."
QDRANT_URL="<your_qdrant_host_url>"
QDRANT_API_KEY="<your_qdrant_api_key>"
OPENAI_MODEL="gpt-4o-mini"
2. InstallationClone the repository and install the required dependencies:git clone <your-repo-link>
cd advanced-research-assistant
# Ensure requirements.txt lists all dependencies from utils.py and prototype_front.py
pip install -r requirements.txt
3. Running the AppExecute the Streamlit application from your terminal:streamlit run prototype_front.py
üåê Live Demo & Visuals[üëâ Click here to view a live demo (If hosted on Streamlit Cloud)]The application is deployed live and fully interactive. The visual above shows the Comparative Analysis feature in action.‚è≠Ô∏è Future EnhancementsUser Authentication: Implement Firebase/Auth0 for user login and private document storage.External Grounding: Integrate Google Search or dedicated web crawlers to allow the LLM to verify facts against the wider internet (External RAG).Asynchronous Processing: Refactor the PDF processing and Qdrant population to run asynchronously to improve UI responsiveness during large uploads.